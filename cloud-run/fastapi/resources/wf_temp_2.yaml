jobs:
  pysparkJob:
    mainPythonFileUri: gs://yt-demo-dev-dataproc-app/spark/simple_spark_job4.py
    args:
      - <WILL-BE-REPLACED>
      - <WILL-BE-REPLACED>
      - <WILL-BE-REPLACED>
      - <WILL-BE-REPLACED>
  stepId: simple-job4
  labels:
    project: yt-demo-dev-dataproc
placement:
  managedCluster:
    clusterName: yt-demo-wf-temp-dev-cluster
    config:
      configBucket: yt-demo-dev-dataproc-staging
      tempBucket: yt-demo-dev-dataproc-temp
      gceClusterConfig:
        zoneUri: europe-west1-b
      masterConfig:
        diskConfig:
          bootDiskSizeGb: 50
        machineTypeUri: n1-standard-4
      softwareConfig:
        imageVersion: '1.4'
        properties:
          dataproc:pip.packages: tokenizers==0.10.1,datasets==1.5.0
      workerConfig:
        diskConfig:
          bootDiskSizeGb: 50
        machineTypeUri: n1-standard-4
        numInstances: 2
parameters:
  - name: SOURCE_BUCKET
    fields:
    - jobs['simple-job4'].pysparkJob.args[0]
  - name: SOURCE_FORMAT 
    fields:
    - jobs['simple-job4'].pysparkJob.args[1]
  - name: DESTINATION
    fields:
    - jobs['simple-job4'].pysparkJob.args[2]
  - name: DESTINATION_NAME
    fields:
    - jobs['simple-job4'].pysparkJob.args[3]